---
title: "Asymptotic Variance"
author: "Guillaume Blanc"
date: "1 septembre 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
```

# Computing the Relative Efficiency of our Estimator

### Computing the Relative Asymptotic Variance of the MLE

To compute the relative asymptotic variance of the estimator, we need:

* the asymptotic variance of the maximum likelihood estimator
* the asymptotic variance of our estimator
* take the ratio of the trace of the latter to the trace of the former

We'll do that for the factor loadings.

### Asymptotic Variance of the MLE

Given a log-likelihood function $L(\theta)$ and its score function $s(Y, \theta)$, the Avar of the MLE can be written as $Avar = I(\theta)^{-1}$ where

$$
I(\theta) = E_{\theta} \left[s(Y, \theta)s(Y, \theta)^\top\right]
$$
To compute this quantity, we'll use the empirical version:

$$
\bar I(\theta) = \frac{1}{n}\sum_{i=1}^n s(Y_i, \theta)s(Y_i, \theta)^\top, 
$$
where $Y_i$ are iid from $F_{\theta}$. We'll use $n$ large enough to reduce the variance of this estimator to reasonable levels for the current application.

We have the following:

\begin{align*}
  s\left(\theta\vert y_i\right) 
    &= \frac{\partial }{\partial \theta}\log \left(\int f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ\right) \\
    &= \frac
    {\int \frac{\partial }{\partial \theta} f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ}
    {\int f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ}\\
    &= \frac
    {\int \frac{\partial }{\partial \theta} \left(\log f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)\right) f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)  \phi(Z)dZ}
    {\int f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ}\\
    &\equiv \frac{N(\theta, y_i)}{D(\theta, y_i)}
\end{align*}


We need the following functions:

* `fYZ`: returns the conditional density $f_{Y|Z}$
* `dlogfYZ`: returns the derivative of the log of the conditional density with respect to the loadings
* `fZ`: returns the density of Z
* `score`: returns the score

Then we need to compute $\bar I(\theta)$, which involves many replications of the above, for each $Y_$, i=1, \dots, n: for each $Y_i$, we need to generate Z and compute the empirical version of $N$ and $D$ above, separately.

$$
\bar N(\theta, y_i) = \frac{1}{n_N}\sum_{i=1}^{n_N}  \frac{\partial }{\partial \theta} \left(\log f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)\right) f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)
$$

$$
\bar D(\theta, y_i) = \frac{1}{n_D}\sum_{i=1}^{n_D}  f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)
$$

Let's write the conditional density $f_{Y|Z}$ again:

$$
f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta) = \prod_{j=1}^p \exp\left(\frac{y_{ij}\eta_{ij} - b_j(\eta_{ij})}{\tau_j} + c_j(y_{ij}, \tau_j)\right)
$$
and its log:

$$
\log f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta) = \sum_{j=1}^p \left(\frac{y_{ij}\eta_{ij} - b_j(\eta_{ij})}{\tau_j} + c_j(y_{ij}, \tau_j)\right),
$$
the derivative of which w.r.t the loadings $\Lambda$ is 

$$
\frac{\partial\log f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta)}{\partial \Lambda} = 
\sum_{j=1}^p \frac{y_{ij}\eta_{ij}' - b_j'(\eta_{ij})\eta_{ij}'}{\tau_j},
$$
where 

* $\eta_{ij}' = \frac{\partial \eta_{ij}}{\partial A} = 
    \begin{pmatrix}
    0 \\
    \vdots\\
    z_i^\top
    \\
    \vdots\\
    0
    \end{pmatrix}$,
  
where the non-zero row is the jth row. For the bernoulli distribution, we have

* $b_j(\eta_{ij}) = \eta_{ij} - \text{sig}(\eta_{ij})$, whith $\text{sig}(x) = 1/(1+\exp(-x))$
* $b_j'(\eta_{ij}) = 1 - \frac{\text{sig}'(\eta_{ij})}{\text{sig}(\eta_{ij})} = \text{sig}(\eta_{ij})$
* $c_j(y_{ij}, \tau_j) = 0$
* $\tau_j=1$

and thus

$$
\frac{\partial\log f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta)}{\partial \Lambda} = 
  y_i z_i^\top - \text{sig}(\eta_i)z_i^\top = (y_i - \text{sig}(\eta_{ij}))z_i^\top,
$$
where (in a slight abuse of notation) the function $\text{sig}(v)$ applies $\text{sig}()$ previously defined element-wise to the vector $v$.


### Function Definitions
To compute everything, we need the following additional functions, that are defined above.

```{r}
sigmoid <- function(x) 1/(1+exp(-x))

# b and c functions
bfunc <- function(natpar){
  natpar - log(sigmoid(natpar))
}
cfunc <- function(Y, psi){
  Y * 0
}


# conditional density of Y (n * p matrix) given Z (n * q matrix), returns an n-vector

fYZ <- function(Y, Z, par){
  natpar <- Z %*% t(par$A) 
  dens <- exp(t(t(Y * natpar - bfunc(natpar))/par$Psi) + cfunc(Y, par$Psi))
  apply(dens, 1, prod)
}

# same as fYZ but with Y a vector, also there is no cfunc... TODO

fYiZ <- function(Yi, Z, par){
  natpar <- Z %*% t(par$A) 
  dens <- exp(t(t(t(Yi * t(natpar)) - bfunc(natpar))/par$Psi))
  apply(dens, 1, prod)
}


# derviative of the log conditional distribution w.r.t A. 
# Returns an n * (p*q) matrix, where each row is a p*q vector,
# which corresponds to the rows of the corresponding p *q matrix of partial derivatives that have been concatenated into a vector. Since there is one such vector per observations, we obtain $n * (p*q)$ matrix in the end

dlogfYZ <- function(Y, Z, par){
  n <- nrow(Y)
  p <- nrow(par$A)
  q <- ncol(par$A)
  natpar <- Z %*% t(par$A)
  t(sapply(1:n, function(i){
    Zimat <- matrix(Z[i,], nrow=p, ncol=q, byrow=T)
    matderiv <- (Y[i,] - sigmoid(natpar[i,])) * Zimat
    vecderiv <- as.vector(t(matderiv))
  }))
}

# same as dlogfYZ but for a single observation
dlogfYiZi <- function(Yi, Zi, par){
  natpar <- as.vector(Zi %*% t(par$A))
  as.vector(t((Yi - sigmoid(natpar)) %*% t(Zi)))
}

# same as dlogfYZ but with vector Y and matrix Z
dlogfYiZ <- function(Yi, Z, par){
  p <- nrow(par$A)
  q <- ncol(par$A)
  n <- nrow(Z)
  natpar <- Z %*% t(par$A)
  t(sapply(1:n, function(i){
    Zimat <- matrix(Z[i,], nrow=p, ncol=q, byrow=T)
    matderiv <- (Yi - sigmoid(natpar[i,])) * Zimat
    vecderiv <- as.vector(t(matderiv))
  }))
}

# Returns D() for each of the observations; returns an n-vector
compute_D <- function(Y, par, nD, seed=NULL){
  n <- nrow(Y)
  q <- ncol(par$A)
  
  if(is.null(seed))set.seed(seed)
  if(n==1){
    D <- mean(sapply(1:nD, function(na){
    Z <- gen_Z(n = n, q=q)
    fYZ(Y, Z, par)
    }))
  } else {
    D <- 
  rowMeans(sapply(1:nD, function(na){
    Z <- gen_Z(n = n, q=q)
    fYZ(Y, Z, par)
    }))
  }
  D
}

# Compute N() for each of the observations, returns an n-vector
compute_N <- function(Y, par, nN, seed=NULL){
  n <- nrow(Y)
  q <- ncol(par$A)
  
  if(is.null(seed))set.seed(seed)
  t(sapply(1:n, function(i){
    Z <- gen_Z(nN, q)
    # dlogFYZ returns an n vector
    colMeans(dlogfYiZ(Y[i,], Z, par) * fYiZ(Y[i,], Z, par))
  }))
}

# finally, compute the scores as a n * (p*q) matrix:
compute_scores <- function(Y, par, nD, nN, seed=NULL){
  N <- compute_N(Y, par, nN, seed)
  D <- compute_D(Y, par, nD, seed)
  N/D
}

# Compute Jacobian:

jacobian <- function(score, theta, eps=1e-3, ...){
  pq <- length(theta)
  jacobian <- matrix(0, pq, pq)
  for(i in 1:pq){
    delta <- rep(0, pq)
    delta[i] <- eps
    jacobian[,i] <- (score(theta+delta, ...) - score(theta - delta, ...))/(2*eps)
  }
  jacobian
}



# or just one score from one observation, as a (p*q) vector:

compute_score <- function(Yi, par, nD, nN, seed=NULL){
  Y <- t(as.vector(Yi))
  N <- compute_N(Y, par, nN, seed=NULL)
  D <- compute_D(Y, par, nD, seed=NULL)
  N/D
}

get_I <- function(scores){
  # center scores
  scores <- scale(scores, scale=F)
  t(scores_Y) %*% scores_Y
  I <- t(scores_Y) %*% scores_Y
  I <- I/nrow(scores_Y)
  I
}

compute_I <- function(par, n, nD, nN, seed=NULL){
  p <- nrow(par$A)
  q <- ncol(par$A)
  gllvm_dat <- gen_gllvm(n, p, q, k=0, family="bernoulli", par=par)
  Y <- gllvm_dat$Y
  scores <- compute_scores(Y, par, nD, nN, seed=NULL)
  I <- get_I(scores)
  Ineg <- solve(I)
  liste(I=I, Ineg =Ineg)
}

# transforms theta from vector to matrix and vice versa
theta_to_vec <- function(theta) as.vector(theta)
theta_to_mat <- function(theta, p, q) matrix(theta, p, q)
```



###  Computing the information matrix for the MLE
This simulation will involve computing expectations of expectations (with respect to different measures), and this at a very high level of precision, which is tremendously computationally expensive. Luckily, our problem allows for substantial speedups, using the fact that the first of these expectation is with respect to a counting measure. The idea is that, with $p$ manifest binary random variables, there are only $2^p$ possible outcomes. So no need to compute all $n$ score values, only these $2^p$ are required, and then we will match them to the realized observations. Of course for this speedup to be possible, we require $p$ to be small: we'll use `p=8`, for `256` outcomes. We'll then generate `n=1e6` observations to produce our Monte Carlo estimates. As explained above, to compute each of the `256` scores,we need to evaluate two expectations with respect to $Z$, again using Monte Carlo. Our empirical tests suggest that $nN=10'000$ and $nD=10'000$ produce good result.

```{r}
p <- 8
# All outcomes, with each row corresponding to 
compute_outcomes <- function(p){
 t(sapply(0:(2**p-1),function(x){ as.integer(intToBits(x))})[p:1, ])
}
outcomes <- compute_outcomes(p)
head(outcomes)
```


Now that we have the outcomes scores, we need to match them to the data $Y$. Notice that the outcomes are conveniently ordered so that the row ith binary value, in base 10, is equal to i. We'll use this.

```{r}
compute_scores_Y <- function(Y, par, nD, nN, seed=NULL, outcomes_scores=NULL){
  n <- nrow(Y)
  p <- nrow(par$A)
  q <- ncol(par$A)
  
  num_outcomes <- 2**p
  
  if(is.null(outcomes_scores)){
    outcomes <- compute_outcomes(p)
    outcomes_scores <- compute_scores(outcomes, par, nD, nN, seed)
  }
  
  bin <- 2^((p-1):0)
  Y.outcomes <- as.vector(Y %*% bin)
  Y.outcomes.freq  <- table(Y.outcomes)
  
  scores_Y <- matrix(0, n, p*q)
  
  for(i in 1:num_outcomes){
    scores_Y[which(Y.outcomes == (i-1)), ] <- matrix(outcomes_scores[i, ],
                                                     nrow=Y.outcomes.freq[i],
                                                     ncol=p*q,
                                                     byrow = T)
  }
  scores_Y
}
```


Now we can generate lots of observations at a relatively low computational cost.
```{r cache=T}
set.seed(1231)
p <- 8 
q <- 1
n <- 1e5
nN <- nD <- 5000
par <- generate_parameters(A = matrix(c(-2, -2, -1, -1, 1, 1, 2, 2), p, 1), B=NULL, phi=NULL, p=p, q=q, k=0)
gllvm <- gen_fastgllvm(n, p, q, family = "binomial", A= par$A, intercept = F, k=0)
Y <- gllvm$Y
par <- gllvm$parameters
par$Psi <- par$phi
scores_Y <- compute_scores_Y(Y, par, nD, nN, seed=42345)
```

```{r}
boxplot(scores_Y, main="Scores at the true value of the parameter.")
points(1:(p*q), colMeans(scores_Y), col=2, pch=1, pty=3)
abline(h=0, col=2)
```
We next compute the information matrix as the empirical average of the outer product of the scores


```{r}
I <- get_I(scores_Y)
```

Finally, we are interested in its inverse, as an estimate of the asymptotic variance of the MLE:

```{r}
Ineg <- solve(I)
Ineg
```


We'll use $\bar I(\theta)^{-1}$ as a benchmark to compare the efficiency of our estimator.

# Asymptotic Variance of our Estimator

Recall that the estimating equations are:

$$
\frac{1}{n}\sum_{i=1}^n \Psi_\Lambda(y_i|z_i^\ast, x_i, \theta) - \mathbb E_Y[\Psi_\Lambda(Y|z_i^\ast, x_i, \theta)] = 0 
$$


$$
\Psi_\Lambda(y_i|z_i, x_i, \theta) = 
    \begin{pmatrix}
    % lambda
        \partial/\partial\theta\lambda_1 \log f_{y_i|z_i}\\
        \vdots\\
        \partial/\partial\theta\lambda_p \log f_{y_i|z_i}\\
    \end{pmatrix}
    =
    \begin{pmatrix}
    % lambda
        \left(y_{i1} - b_1'(\eta_{i1})\right)z_i/\tau_1\\
        \vdots\\
        \left(y_{ip} - b_1'(\eta_{ip})\right)z_i/\tau_p\\
    \end{pmatrix}
    .
$$

We need:

* `fYZ`: same as above
* `compute_Z`: compute Z given Y and theta

```{r}

compute_Z <- function(Y, par){
  K <- compute_K(par)
  Z <- Y %*% t(K)
}

compute_profile_scores <- function(Y, par, nE=nrow(Y), seed=NULL){
  p <- nrow(par$A)
  q <- ncol(par$A)
  
  Zstar <- compute_Z(Y, par)
  deriv <- dlogfYZ(Y, Zstar, par)
  # compute expectation under par_true
  if(!is.null(seed)) set.seed(seed)
  Y0 <- gen_gllvm(nE, p, q, family="bernoulli", par=par)$Y
  Zstar0 <- compute_Z(Y0, par)
  deriv0 <- dlogfYZ(Y0, Zstar0, par)
  deriv0.mean <- colMeans(deriv0)
  
  t(t(deriv) - deriv0.mean)
}

variance_scores <- function(Y, par, nE){
  scores <- compute_profile_scores(Y, par, nE)
  n <- nrow(scores)
  t(scores) %*% scores/n
}

score_to_derive <- function(Y, theta, par, seed=NULL){
  p <- nrow(par$A)
  q <- ncol(par$A)
  par2 <- par
  par2$A <- theta_to_mat(theta, p, q)
  colMeans(compute_profile_scores(Y, par2, nE, seed=seed))
}
```

```{r cache=T}
nE <- n
theta <- theta_to_vec(par$A)


# now we need the derivative

A <- - jacobian(score_to_derive, theta, eps=1e-1, Y=Y, par=par, seed=123123)
# A2 <- - numDeriv::jacobian(score_to_derive, theta, method.args=list(eps=1e-1))
B <- variance_scores(Y, par, nE)
# now compute the avar
Aneg <- solve(A)
Avar <- Aneg %*% B %*% t(Aneg)
```
## Computing the Relative Efficiency
We can now  compute the relative efficiency as the following ratio:

$$
\text{ARE} = \frac{\text{trace}(I^{-1})}{\text{trace}\left(A(\theta)^{-1}B(\theta)\left[A(\theta)^{-1}\right]^\top\right)}
$$

```{r}
ARE = sum(diag(Ineg))/sum(diag(Avar))
```

And we obtain an ARE of `r ARE` for this example. Of course, due to the Monte Carlo simulations, this result is random. One could repeat this experiment to test the hypothesis that this ratio is greater than 1.
