---
title: "Asymptotic Variance"
author: "Guillaume Blanc"
date: "1 septembre 2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
devtools::load_all()
```

## Computing the Relative Asymptotic Variance of the MLE

To compute the relative asymptotic variance of the estimator, we need:

* the asymptotic variance of the maximum likelihood estimator
* the asymptotic variance of our estimator
* take the ratio of the trace of the latter to the trace of the former

We'll do that for the factor loadings.

### Asymptotic Variance of the MLE

Given a log-likelihood function $L(\theta)$ and its score function $s(Y, \theta)$, the Avar of the MLE can be written as $Avar = I(\theta)^{-1}$ where

$$
I(\theta) = E_{\theta} \left[s(Y, \theta)s(Y, \theta)^\top\right]
$$
To compute this quantity, we'll use the empirical version:

$$
\bar I(\theta) = \frac{1}{n}\sum_{i=1}^n s(Y_i, \theta)s(Y_i, \theta)^\top, 
$$
where $Y_i$ are iid from $F_{\theta}$. We'll use $n$ large enough to reduce the variance of this estimator to reasonable levels for the current application.

We have the following:

\begin{align*}
  s\left(\theta\vert y_i\right) 
    &= \frac{\partial }{\partial \theta}\log \left(\int f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ\right) \\
    &= \frac
    {\int \frac{\partial }{\partial \theta} f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ}
    {\int f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ}\\
    &= \frac
    {\int \frac{\partial }{\partial \theta} \left(\log f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)\right) f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)  \phi(Z)dZ}
    {\int f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)        \phi(Z)dZ}\\
    &\equiv \frac{N(\theta, y_i)}{D(\theta, y_i)}
\end{align*}


We need the following functions:

* `fYZ`: returns the conditional density $f_{Y|Z}$
* `dlogfYZ`: returns the derivative of the log of the conditional density with respect to the loadings
* `fZ`: returns the density of Z
* `score`: returns the score

Then we need to compute $\bar I(\theta)$, which involves many replications of the above, for each $Y_$, i=1, \dots, n: for each $Y_i$, we need to generate Z and compute the empirical version of $N$ and $D$ above, separately.

$$
\bar N(\theta, y_i) = \frac{1}{n_N}\sum_{i=1}^{n_N}  \frac{\partial }{\partial \theta} \left(\log f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)\right) f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)
$$

$$
\bar D(\theta, y_i) = \frac{1}{n_D}\sum_{i=1}^{n_D}  f_{Y_{i}|Z}(y_{i}|Z, x_i, \theta)
$$

Let's write the conditional density $f_{Y|Z}$ again:

$$
f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta) = \prod_{j=1}^p \exp\left(\frac{y_{ij}\eta_{ij} - b_j(\eta_{ij})}{\tau_j} + c_j(y_{ij}, \tau_j)\right)
$$
and its log:

$$
\log f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta) = \sum_{j=1}^p \left(\frac{y_{ij}\eta_{ij} - b_j(\eta_{ij})}{\tau_j} + c_j(y_{ij}, \tau_j)\right),
$$
the derivative of which w.r.t the loadings $\Lambda$ is 

$$
\frac{\partial\log f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta)}{\partial \Lambda} = 
\sum_{j=1}^p \frac{y_{ij}\eta_{ij}' - b_j'(\eta_{ij})\eta_{ij}'}{\tau_j},
$$
where 

* $\eta_{ij}' = \frac{\partial \eta_{ij}}{\partial A} = 
    \begin{pmatrix}
    0 \\
    \vdots\\
    z_i^\top
    \\
    \vdots\\
    0
    \end{pmatrix}$

and where, for the bernoulli distribution, we have

* $b_j(\eta_{ij}) = \eta_{ij} - \text{sig}(\eta_{ij})$, whith $\text{sig}(x) = 1/(1+\exp(-x))$
* $b_j'(\eta_{ij}) = 1 - \frac{\text{sig}'(\eta_{ij})}{\text{sig}(\eta_{ij})} = \text{sig}(\eta_{ij})$
* $c_j(y_{ij}, \tau_j) = 0$
* $\tau_j=1$

and thus

$$
\frac{\partial\log f_{Y_{i}|Z_i}(y_{i}\vert Z_i,x_i,\theta)}{\partial \Lambda} = 
  y_i z_i^\top - \text{sig}(\eta_i)z_i^\top = (y_i - \text{sig}(\eta_{ij}))z_i^\top,
$$
where the vector function $\text{sig}(v)$ applies $\text{sig}()$ element-wise to the vector $v$.


### Function Definitions
To compute everything, we need the following additional functions, that are defined above.

```{r}
# b and c functions
bfunc <- function(natpar){
  natpar - log(sigmoid(natpar))
}
cfunc <- function(Y, psi){
  Y * 0
}


# conditional density of Y (n * p matrix) given Z (n * q matrix), returns an n-vector

fYZ <- function(Y, Z, par){
  natpar <- Z %*% t(par$A) 
  dens <- exp(t(t(Y * natpar - bfunc(natpar))/par$Psi) + cfunc(Y, par$Psi))
  apply(dens, 1, prod)
}

# same as fYZ but with Y a vector, also there is no cfunc... TODO

fYiZ <- function(Yi, Z, par){
  natpar <- Z %*% t(par$A) 
  dens <- exp(t(t(t(Yi * t(natpar)) - bfunc(natpar))/par$Psi))
  apply(dens, 1, prod)
}


# derviative of the log conditional distribution w.r.t A. 
# Returns an n * (p*q) matrix, where each row is a p*q vector,
# which corresponds to the rows of the corresponding p *q matrix of partial derivatives that have been concatenated into a vector. Since there is one such vector per observations, we obtain $n * (p*q)$ matrix in the end

dlogfYZ <- function(Y, Z, par){
  n <- nrow(Y)
  p <- nrow(par$A)
  q <- ncol(par$A)
  natpar <- Z %*% t(par$A)
  t(sapply(1:n, function(i){
    Zimat <- matrix(Z[i,], nrow=p, ncol=q, byrow=T)
    matderiv <- (Y[i,] - sigmoid(natpar[i,])) * Zimat
    vecderiv <- as.vector(t(matderiv))
  }))
}

# same as dlogfYZ but for a single observation
dlogfYiZi <- function(Yi, Zi, par){
  natpar <- as.vector(Zi %*% t(par$A))
  as.vector(t((Yi - sigmoid(natpar)) %*% t(Zi)))
}

# same as dlogfYZ but with vector Y and matrix Z
dlogfYiZ <- function(Yi, Z, par){
  p <- nrow(par$A)
  q <- ncol(par$A)
  n <- nrow(Z)
  natpar <- Z %*% t(par$A)
  t(sapply(1:n, function(i){
    Zimat <- matrix(Z[i,], nrow=p, ncol=q, byrow=T)
    matderiv <- (Yi - sigmoid(natpar[i,])) * Zimat
    vecderiv <- as.vector(t(matderiv))
  }))
}

# Returns D() for each of the observations; returns an n-vector
compute_D <- function(Y, par, nD){
  n <- nrow(Y)
  q <- ncol(par$A)
  
  if(n==1){
    D <- mean(sapply(1:nD, function(na){
    Z <- gen_Z(n = n, q=q)
    fYZ(Y, Z, par)
    }))
  } else {
    D <- 
  rowMeans(sapply(1:nD, function(na){
    Z <- gen_Z(n = n, q=q)
    fYZ(Y, Z, par)
    }))
  }
  D
}

# Compute N() for each of the observations, returns an n-vector
compute_N <- function(Y, par, nN){
  n <- nrow(Y)
  q <- ncol(par$A)
  
  # take the same Z for each observation (shouldn't matter much)
  Z <- gen_Z(nN, q)
  # for each observation, compute nN realizations of the derivative of the log density and take their average. The result is a n* (p*q) matrix of empirical averages of the quantity. This is INEFFICIENT, but since we only compute this once, it should be ok...
  A.deriv <- 
    t(sapply(1:n, function(i){
      rowMeans(sapply(1:nN, function(iN){
        dlogfYiZi(Y[i,], Z[iN, ], par) * fYZ(Y[i,,drop=F], Z[iN,,drop=F], par)
      }))
    }))
  A.deriv
}

# finally, compute the scores as a n * (p*q) matrix:
compute_scores <- function(Y, par, nD, nN){
  N <- compute_N(Y, par, nN)
  D <- compute_D(Y, par, nD)
  N/D
}

# or just one score from one observation, as a (p*q) vector:

compute_score <- function(Yi, par, nD, nN){
  Y <- t(as.vector(Yi))
  N <- compute_N(Y, par, nN)
  D <- compute_D(Y, par, nD)
  N/D
}

compute_I <- function(par, n, nD, nN){
  p <- nrow(par$A)
  q <- ncol(par$A)
  gllvm_dat <- gen_gllvm(n, p, q, k=0, family="bernoulli", par=par)
  Y <- gllvm_dat$Y
  scores <- compute_scores(Y, par, nD, nN)
  
  I <- matrix(rowMeans(sapply(1:n, function(i){
      as.vector(scores[i,] %*% t(scores[i,]))
      })), p*q, p*q)
  I
}
```



###  Computing the information matrix for the MLE


```{r}
set.seed(451)
# generate the parameters 
n <- 1000
nD <- nN <- 100
p <- 3
q <- 2
par <- gen_par(p, q, k=0, family="bernoulli")

gllvm_dat <- gen_gllvm(n, p, q, k=0, family="bernoulli", par=par)
Y <- gllvm_dat$Y
Z <- gllvm_dat$Z

```

With $p$ manifest random variables, there are only $2^p$ possible outcomes. So no need to compute all $n$ gradients, only these $2^p$. 

```{r}
# All outcomes, with each row corresponding to 
compute_outcomes <- function(p){
 t(sapply(0:(2**p-1),function(x){ as.integer(intToBits(x))})[p:1, ])
}
outcomes <- compute_outcomes(p)
outcomes
```

#### Testing the functions


```{r}
all.equal(fYZ(Y, Z, par), fYZ2(Y, Z, par))
```
```{r}
all.equal(dlogfYZ(Y, Z, par), dlogfYZ2(Y, Z, par))
```

```{r}
compute_D2 <- function(Y, par, nD){
  p <- nrow(par$A)
  q <- ncol(par$A)
  n <- nrow(Y)
  sapply(1:n, function(i){
    Z <- gen_Z(nD, q)
    Ymat <- matrix(Y[i,], nrow=nD, ncol=p, byrow=T)
    mean(fYZ(Ymat, Z, par))
  })
}
```

```{r}
D1 <- compute_D(Y, par, nD)
D2 <- compute_D2(Y, par, nD)
```

```{r}
compute_N2 <- function(Y, par, nN){
  n <- nrow(Y)
  q <- ncol(par$A)
  t(sapply(1:n, function(i){
    Z <- gen_Z(nN, q)
    # dlogFYZ returns an n vector
    colMeans(dlogfYiZ(Y[i,], Z, par) * fYiZ(Y[i,], Z, par))
  }))
}
```

```{r, cache=T}
# these two have been checked to lead to similar results but N2 is way faster
N1 <- compute_N(outcomes, par, nN)
N2 <- compute_N2(outcomes, par, nN)
plot(N1, N2)
```



```{r}
compute_scores2 <- function(Y, par, nD, nN){
  N <- compute_N2(Y, par, nN)
  D <- compute_D2(Y, par, nD)
  N/D
}
```

```{r, cache=T}
# compute_scores2 is way faster. both functions have been checked to yield identical results.
outcomes_scores1 <- compute_scores(outcomes, par, 10000, 10000)
outcomes_scores2 <- compute_scores2(outcomes, par, 10000, 10000)

plot(outcomes_scores1, outcomes_scores2)
```

We're not done yet! Now that we have the outcomes scores, we need to match them to the data $Y$. Notice that the outcomes are conveniently ordered so that the row ith binary value, in base 10, is equal to i. We'll use this.

```{r}
compute_scores_Y <- function(Y, par, nD, nN, outcomes_scores=NULL){
  n <- nrow(Y)
  p <- nrow(par$A)
  q <- ncol(par$A)
  
  num_outcomes <- 2**p
  
  if(is.null(outcomes_scores)){
    outcomes <- compute_outcomes(p)
    outcomes_scores <- compute_scores2(outcomes, par, nD, nN)
  }
  
  bin <- 2^((p-1):0)
  Y.outcomes <- as.vector(Y %*% bin)
  Y.outcomes.freq  <- table(Y.outcomes)
  
  scores_Y <- matrix(0, n, p*q)
  
  for(i in 1:num_outcomes){
    scores_Y[which(Y.outcomes == (i-1)), ] <- matrix(outcomes_scores[i, ],
                                                     nrow=Y.outcomes.freq[i],
                                                     ncol=p*q,
                                                     byrow = T)
  }
  scores_Y
}
```


Now we can generate lots of observations at minimal computational cost, although it's approximate.
```{r}
set.seed(1231)
p <- 5 
q <- 1
n <- 1e5
nN <- nD <- 10000
gllvm <- gen_gllvm(n, p, q, family = "bernoulli")
Y <- gllvm$Y
par <- gllvm$par
set.seed(24124)
scores_Y <- compute_scores_Y(Y, par, nD, nN)
```

```{r}
boxplot(scores_Y, main="Scores at the true value of the parameter.")
points(1:(p*q), colMeans(scores_Y), col=2, pch=1, pty=3)
abline(h=0, col=2)
```
We next compute the information matrix as the empirical average of the outer product of the scores


```{r, cache=T}
I <- matrix(rowMeans(sapply(1:n, function(i){
  as.vector(scores_Y[i,] %*% t(scores_Y[i,]))
})), p*q, p*q)
```

or equivalently use the wrapper function `compute_I`:

```{r, eval=FALSE}
# not updated yet, do not run
# I <- compute_I(par, n, nD, nN)
```

# Now we compute 

Finally, we are interested in its inverse, as an estimate of the asymptotic variance of the MLE:

```{r}
Ineg <- solve(I)
Ineg
```


We'll use `Ineg` as a benchmark to compare other estimators. For instance, we could compare the traces, which is here equal to `r sum(diag(Ineg))`, or compute the relative efficiency of the estimators.



## Now try with numDeriv
As a check, we now compute the asymptotic variance using the expectation of the Hessian:
```{r cache=T}
theta <- as.vector(par$A)
# Y <- generator_MLE(theta, 1000)
mean_scores <- function(theta){
  set.seed(21314)
  par2 <- par
  par2$A <- theta_to_mat(theta, p, q)
  colMeans(compute_scores_Y(Y, par2, nD, nN))
}
nD <- 10000
nN <- 10000
H <- - numDeriv::jacobian(mean_scores, theta, method.args=list(eps=1e-001))
```

They're very similar, which is a good sign that the derivations and implementations were correct.

```{r}
plot(I, H)
```


# Asymptotic Variance of our Estimator

Recall that the estimating equations are:

$$
\frac{1}{n}\sum_{i=1}^n \Psi_\Lambda(y_i|z_i^\ast, x_i, \theta) - \mathbb E_Y[\Psi_\Lambda(Y|z_i^\ast, x_i, \theta)] = 0 
$$


$$
\Psi_\Lambda(y_i|z_i, x_i, \theta) = 
    \begin{pmatrix}
    % lambda
        \partial/\partial\theta\lambda_1 \log f_{y_i|z_i}\\
        \vdots\\
        \partial/\partial\theta\lambda_p \log f_{y_i|z_i}\\
    \end{pmatrix}
    =
    \begin{pmatrix}
    % lambda
        \left(y_{i1} - b_1'(\eta_{i1})\right)z_i/\tau_1\\
        \vdots\\
        \left(y_{ip} - b_1'(\eta_{ip})\right)z_i/\tau_p\\
    \end{pmatrix}
    .
$$

We need:

* `fYZ`: same as above
* `compute_Z`: compute Z given Y and theta

```{r}

compute_Z <- function(Y, par){
  K <- compute_K(par)
  Z <- Y %*% t(K)
}

compute_profile_scores <- function(Y, par, nE=nrow(Y)){
  set.seed(12323)
  p <- nrow(par$A)
  q <- ncol(par$A)
  Zstar <- compute_Z(Y, par)
  deriv <- dlogfYZ(Y, Zstar, par)
  # compute expectation under par_true
  Y0 <- gen_gllvm(nE, p, q, family="bernoulli", par=par)$Y
  Zstar0 <- compute_Z(Y0, par)
  deriv0 <- dlogfYZ(Y0, Zstar0, par)
  deriv0.mean <- colMeans(deriv0)
  
  t(t(deriv) - deriv0.mean)
}

variance_scores <- function(scores){
  n <- nrow(scores)
  scores <- scale(scores, scale=F)
  t(scores) %*% scores/n
}

score_to_derive <- function(theta){
  p <- nrow(par$A)
  q <- ncol(par$A)
  par2 <- par
  par2$A <- theta_to_mat(theta, p, q)
  colMeans(compute_profile_scores(Y, par2, nE))
}

n <- 1e4
nE <- 1e4
p <- 5
q <- 1

gllvm <- gen_gllvm(n, p, q, family="bernoulli")
par <- gllvm$par
Y   <- gllvm$Y
Z   <- gllvm$Z
prof_scores_Y <- compute_profile_scores(Y, par)


theta <- as.vector(par$A)
aseq <- seq(-1,1, l=21)
sims <- sapply(aseq, function(delta){
  theta2 <- theta
  cat("\n", delta)
  theta2[i] <- theta[i] + delta
  score_to_derive(theta2)
})
plot(aseq, sims[i,])
# now we need the derivative
A <- - numDeriv::jacobian(score_to_derive, theta, method.args=list(eps=1e-1))
B <- variance_scores(prof_scores_Y)
# now compute the avar
Aneg <- solve(A)
Aneg %*% B %*% t(Aneg)
```


### Simultaneous-perturbation method to compute the derivative of a score

This is a reminder of Spall's method: we need a generator of data from the true model, a score function, and a point $\theta$, and it returns the derivative.

* Generator is a function that takes `theta` and returns a matrix whose rows are the realizations from the true model.
* Score is a function that takes `theta` and an observation $Y_i$ (a vector), and returns a p-vector
* `theta` is a p*q-vector of parameters
* `N` is the number of summands to compute the final hessian.
  
  
#### Example with multivariate gaussian variables
```{r}
generator_MVNORM <- function(theta, N){
  mvtnorm::rmvnorm(N, mean=theta, sigma=Sigma)
}

score_MVNORM_MLE <- function(theta, Y){
  if(!is.vector(Y)) stop("Y must be provided as a vector, not a matrix.")
  as.vector(- Sigma_inverse %*% (Y - theta))
}

set.seed(2131)
N <- 1000
theta <- 1:p
# add some randomness
Sigma <- diag(2, p)
Sigma <- rWishart(1, p, diag(p))[,,1]/sqrt(p)
Sigma_inverse <- solve(Sigma)

H.hat <- compute_expected_hessian(generator=generator_MVNORM, score=score_MVNORM_MLE, theta=theta, N=N, c=1e-04)$H

# This should be close to Sigma_inverse

par(mfrow=c(1,3))
image(Sigma_inverse[,p:1])
image(H.hat[,p:1])
plot(H.hat, Sigma_inverse); abline(a=0, b=1, col=2)
par(mfrow=c(1,1))
```

#### Now with our example 
```{r}

theta_to_vec <- function(theta) as.vector(theta)
theta_to_mat <- function(theta, p, q) matrix(theta, p, q)

generator_MLE <- function(theta, N){
  p <- nrow(par$A)
  q <- ncol(par$A)
  par$A <- theta_to_mat(theta, p, q)
  Y <- gen_gllvm(N, p, q, k=0, family="bernoulli", par=par)$Y
  if(N==1) Y <- t(as.vector(Y))
  Y
}

score_MLE <- function(theta, Y){
  p <- nrow(par$A)
  q <- ncol(par$A)
  par$A <- theta_to_mat(theta, p, q)
  as.vector(compute_score(Y, par, nD, nN))
}

N <- 10
nD <- nN <- 1000
set.seed(1324)
theta <- theta_to_vec(par$A)
# test:
Y <- generator_MLE(theta=theta, N)

H <- compute_expected_hessian(generator=generator_MLE, score=score_MLE, theta=theta, N=N, c=1, score.seed = T)$H
```

# variance with the same Y?

```{r}
nD <- 1000
nN <- 1000
```
