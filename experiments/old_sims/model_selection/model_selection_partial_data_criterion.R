devtools::load_all()
library(caret)

# Find Criterion
get_criterion_factory <- function(Y,X, method="BIC", family="gaussian", prop.data=1){
  stopifnot(prop.data<=1 && prop.data >0)
  if(method=="BIC"){
    get_criterion <- function(models){
      crit <- apply(models, 1, function(model){
        fit <- glm(Y~0 + X[, model,drop=F], family=family)
        mse <- (fit$y-fit$fitted.values)^2
        mse <- mean(sort(mse)[1:(length(mse)*prop.data)]) + sqrt((sum(model))^2)/ncol(X)/20
        mse
      })
      list(crit=crit)
    }
  }
  return(get_criterion)
}

# increase weights of the model by repeating them

weight_models <- function(M, BIC){
  M <- M[order(BIC),]
  n <- nrow(M)
  # repeat the best!
  id <- unlist(sapply(1:n, function(i) rep(i, max(n/2-i**1.1, 1))))
  M <- M[id,]
}

# ensemble method for model selection generated by the GLLVM model

# function to return the models that perform best, return only unique models
# M : matrix of models
# TODO: TEST whether the crit is deterministic! it needs to be ! e.g. CV with same seed
# we want to have the LOWEST VALUE HERE

get_best_models <- function(crit.value, M, n, unique=T){
  ord <- order(crit.value)
  keep.id <- 1:n
  if(unique){
    keep.id[1] <- ord[1]
    # loop through all the models until M is filled with unique models or we're out of models to check
    # index the number of retained unique models
    j <- 2
    for(i in 2:nrow(M)){
      # check if 2 models are the same (thus have same crit.value)
      if(crit.value[ord[i]]==crit.value[ord[i-1]]){
        # check if it indeed the same model, in which case we skip it since we want unique models
        if(mean(M[ord[i],] == M[ord[i-1],])==1){
          next()
        } else {
          keep.id[j] <- ord[i]
          j <- j + 1
        }
      } else {
        keep.id[j] <- ord[i]
        j <- j + 1
      }
      if(j==(n+1)) break()
    }

    if(j!=(n+1)) warning(paste0("Could not find ", n, " different models."))
  } else {
    keep.id <- ord[1:n]
  }
  list(M=M[keep.id,], crit.value=crit.value[keep.id])
}



# Function trying to maximize a criterion via iterative GLLVM fits...
# init must be a list with all the useful quantities...
modelselec <- function(Y, X, M=NULL, fit.init=NULL, family="gaussian", get_criterion=NULL, mod.gen=100, mod.keep=20, init=NULL, iter=100, prop.active.init=0.2, unique=T, prop.data=1){
  p <- ncol(X)
  fit <- fit.init
  if(is.null(get_criterion)) get_criterion  <- get_criterion_factory(Y, X, method="BIC", family=family, prop.data = prop.data)

  if(is.null(M) & is.null(fit)){
    M <- t(sapply(1:(mod.gen), function(na){
      s <- sample(p, sample(min(n, p) * prop.active.init ,1), replace=F)
      (1:p) %in% s
    }))
    BIC <- get_criterion(M)$crit
    Mbest <- get_best_models(BIC, M, mod.keep)
    M <- Mbest$M
    BIC <- Mbest$crit

    fit <- fastgllvm_oop(rbind(1, M), X=1, q=q, maxit=100, verbose=F, family=binomial())

  } else if(is.null(M) & !is.null(fit)){
    M <- simulate_fastgllvm(fit, n=mod.keep)$Y > 0
    BIC <- get_criterion(M)$crit
  } else if(!is.null(M) & is.null(fit)){
    fit <- fastgllvm_oop(rbind(1, M), X=1, q=q, maxit=100, verbose=F, family=binomial())
  } else if(!is.null(M) & !is.null(fit)){
    BIC <- get_criterion(M)$crit
  }

  BIC.hist <- matrix(0, iter, mod.keep)
  M.hist <- matrix(0, iter*p, mod.keep)

  BIC.hist[1,] <- BIC
  M.hist[1:p, ] <- M


  for(i in 2:iter){
    M.old <- M
    # BIC.old <- get_criterion(M.old)$crit # TODO: change that back to BIC.old <- BIC
    BIC.old <- BIC

    M <- simulate_fastgllvm(fit, mod.gen)$Y > 0
    M <- M[rowSums(M)< (min(p, n)) & rowSums(M) >1, ] # only take valid models
    BIC <- get_criterion(M)$crit
    # take the mod.keep best
    Mbest <- get_best_models(c(BIC.old, BIC), rbind(M.old, M), mod.keep)
    M <- Mbest$M
    BIC <- Mbest$crit

    #TODO: troubleshooting: covariance cannot be 0!!!
    # if(any(colSums(M)==0))M <- rbind(M, colSums(M)==0)
    # if(any(colSums(M)==1))M <- rbind(M, colSums(M)==1)
    M <- rbind(M, 0)
    M <- rbind(M, 1)
    ######################

    #weight models
    # M <- weight_models(M, BIC)

    fit <- fastgllvm_oop(M, q=1, X=1, family=binomial(), method="SA", A.init= fit$A, B.init=fit$B, phi.init = fit$phi, maxit = 10, learning_rate.args = list(start=5, end=.1), verbose=F)
    M <- M[-c(nrow(M)-1, nrow(M)),]

    # M.hist[p*(i-1) + (1:p),] <- M
    BIC.hist[i,] <- BIC

    # print some stuff
    cat("\niter:", i, "BIC reached:", min(BIC), " worst: ", max(BIC), " num active :", mean(rowSums(M)))
    # boxplot(BIC)
  }
  list(Best=M, BIC=BIC, BIC.hist = BIC.hist, fit=fit)
}


# Generate some data
set.seed(2131)
n <- 1000
p <- 100
q <- 1
beta0 <- c(rep(-2,4), rep(2,4), rep(0, p-8))
model.true <- t(beta0 != 0)
unique <- F
prop.data <-1
family <- binomial(link = "logit")

X0 <- matrix(rnorm(n*p), n, p)
linpar0 <- X0 %*% beta0
Y0 <- rbinom(n, 1, 1/(1+exp(-linpar0)))


beta01 <- c(rep(-2,4), rep(2,4), rep(0, p-8))
beta02 <- c(rep(-2,4), rep(2,4), rep(0, p-8))
beta02 <- c(rep(0, p-8), rep(-2,4), rep(2,4))

trueModel01 <- t(beta01!=0)
trueModel02 <- t(beta02!=0)

linpar1 <- X0 %*% beta01
linpar2 <- X0 %*% beta02
s <- sample(n)
s <- 1:n
linpar <- c(linpar1[s[1:(n/2)]], linpar2[s[(n/2+1):n]])
hist(family$linkinv(linpar))


get_criterion <- get_criterion_factory(Y0, X0, method="BIC", family=family, prop.data=.2)

BIC.true <- get_criterion(trueModel01)
M.random <- t(sapply(1:10, function(na){
  s <- sample(p, 9, replace=F)
  ((1:p) %in% s) > 0
}))
a <- get_criterion(rbind(trueModel01, trueModel02, M.random))

X0 <- X0[s,]

Y0 <- rbinom(n, 1,1/(1+exp(-linpar)))
# add another half from another model

selec <- modelselec(Y0, X0, M=NULL, fit.init=NULL, mod.gen = 100, mod.keep=50, prop.active.init = .5, iter=20, family=family, unique=unique, prop.data = .6)


# we can continue the fit here...
selec <- modelselec(Y0, X0, M=selec$Best,  fit.init=selec$fit, mod.gen = 100, mod.keep=50, prop.active.init = .5, iter=20, family=family, unique=unique, prop.data = .6)


# fit <- fastgllvm_oop(selec$Best, q=2, X=1, maxit=1000, tol=0, family=binomial())
fit <- selec$fit
Msim <- simulate_fastgllvm(fit, 10000)$Y
plot(colMeans(Msim), colMeans(selec$Best))


boxplot(t(selec$BIC.hist), main="boxplots of BICs as a function of runs.")
abline(h=BIC.true, col=2)
plot(colMeans(selec$Best))
plot_fastgllvm(selec$fit)
